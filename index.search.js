var relearn_search_index=[{content:` Kubernetes est une solution d’orchestration de conteneurs extrêmement populaire. Le projet est très ambitieux : une façon de considérer son ampleur est de voir Kubernetes comme un système d’exploitation (et un standard ouvert) pour les applications distribuées et le cloud. Le projet est développé en Open Source au sein de la Cloud Native Computing Foundation. Architecture de Kubernetes Kubernetes rassemble en un cluster et fait coopérer un groupe de serveurs appelés noeuds (nodes).
Kubernetes a une architecture Control Plane/workers composée d’un control plane et de nœuds de calculs (workers).
Cette architecture permet essentiellement de rassembler les machines en un cluster unique sur lequel on peut faire tourner des “charges de calcul” (workloads) très diverses.
Sur un tel cluster le déploiement d’un workload prend la forme de ressources (objets k8s) qu’on décrit sous forme de code et qu’on crée ensuite effectivement via l’API Kubernetes.
Pour uniformiser les déploiement logiciel Kubernetes est basé sur le standard des conteneurs (défini aujourd’hui sous le nom Container Runtime Interface, Docker est l’implémentation la plus connue).
Plutôt que de déployer directement des conteneurs, Kubernetes crée des aggrégats de un ou plusieurs conteneurs appelés des Pods. Les pods sont donc l’unité de base de Kubernetes.
Philosophie Historique et popularité Kubernetes est un logiciel développé originellement par Google et basé sur une dizaine d’années d’expérience de déploiement d’applications énormes (distribuées) sur des clusters de machines.
Maintenant à part entière au sein de Cloud Native, son ancêtre est l’orchestrateur borg utilisé par Google dans les années 2000.
La première version est sortie en 2015 et k8s est devenu depuis l’un des projets open source les plus populaires du monde.
L’écosystème logiciel de Kubernetes s’est développée autour la Cloud Native Computing Foundation qui comprend notamment : Google, CoreOS, Mesosphere, Red Hat, Twitter, Huawei, Intel, Cisco, IBM, Docker, Univa et VMware. Cette fondation vise au pilotage et au financement collaboratif du développement de Kubernetes (un peut comme la Linux Foundation).
Les 3 gros changements Kubernetes se trouve au coeur de trois transformations profondes techniques, humaines et économiques de l’informatique:
Le cloud La conteneurisation logicielle Le mouvement DevOps Il est un des projets qui symbolise et supporte techniquement ces transformations. D’où son omniprésence dans les discussions informatiques actuellement.
Le Cloud Le cloud est un mouvement de réorganisation technique et économique de l’informatique. On retourne à la consommation de “temps de calcul”, pour remplacer le serveur dédié. Pour organiser cela on définit trois niveaux, techniques et économiques: Software as a Service: location de services à travers internet pour les usagers finaux Plateforme as a Service: location d’un environnement d’exécution logiciel flexible à destination des développeurs Infrastructure as a Service: location de resources “matérielles” à la demande pour installer des logiciels sans avoir à maintenir un data center. Conteneurisation La conteneurisation est permise par l’isolation au niveau du noyau du système d’exploitation du serveur : les processus sont isolés dans des namespaces au niveau du noyau. Cette innovation permet de simuler l’isolation sans ajouter une couche de virtualisation comme pour les machines virtuelles.
Ainsi les conteneurs permettent d’avoir des performances proche d’une application traditionnelle tournant directement sur le système d’exploitation hote et ainsi d’optimiser les ressources.
Les images de conteneurs sont aussi beaucoup plus légers qu’une image de VM.
Les technologies de conteneurisation permettent donc de faire des boîtes isolées avec les logiciels pour apporter l’uniformisation du déploiement:
Une façon standard de packager un logiciel. Cela permet d’assembler de grosses applications comme des briques Cela réduit la complexité grâce: à l’intégration de toutes les dépendance déjà dans le conteneur au principe d’immutabilité qui implique de jeter les conteneurs ( Pets vs Cattle ) ce qui rend l’infra prédictible. DevOps Faire le pont entre les développeurs et les administrateurs système. Calquer les rythmes de travail sur l’organisation agile du développement logiciel Rapprocher techniquement la gestion de l’infrastructure du développement avec l’infrastructure as code. concrêtement on écrit des fichiers de code pour gérer les éléments d’infra l’état de l’infrastructure est plus claire et documentée par le code la complexité est plus gérable car tout est déclaré et modifiable au fur et à mesure de façon centralisée l’usage de git et des branches/tags pour la gestion de l’évolution d’infrastructure Apports techniques de Kubernetes pour le DevOps Abstraction et standardisation des infrastructures:
Langage descriptif et incrémental: on décrit ce qu’on veut plutôt que la logique complexe pour l’atteindre Logique opérationnelle intégrée dans l’orchestrateur: la responsabilité de l’état du cluster est laissé au controlleur k8s ce qui simplifie le travail On peut alors espérer fluidifier la gestion des défis techniques d’un grosse application et atteindre plus ou moins la livraison logicielle continue (CD de CI/CD)
Architecture logicielle optimale pour Kubernetes Kubernetes est très versatile et permet d’installer des logiciels traditionnels “monolithiques” (gros backends situés sur une seule machine).
Cependant aux vues des transformations humaines et techniques précédentes, l’organisation de Kubernetes prend vraiment sens pour le développement d’applications microservices:
des applications avec de nombreux de “petits” services. chaque service a des problématiques très limitées (gestion des factures = un logiciel qui fait que ça) les services communiquent par le réseaux selon différents modes/API (REST, gRPC, job queues, GraphQL) Les microservices permettent justement le DevOps car:
ils peuvent être déployés séparéments une petite équipe gère chaque service ou groupe thématique de services Nous y reviendrons pour expliquer l’usage des ressources Kubernetes.
Objets fondamentaux de Kubernetes Les pods Kubernetes servent à grouper des conteneurs fortement couplés en unités d’application Les deployments sont une abstraction pour créer ou mettre à jour (ex : scaler) des groupes de pods. Enfin, les services sont des points d’accès réseau qui permettent aux différents workloads (deployments) de communiquer entre eux et avec l’extérieur. Au delà de ces trois éléments, l’écosystème d’objets de Kubernetes est vaste et complexe
Kubernetes entre Cloud et auto-hébergement Un des intérêts principaux de Kubernetes est de fournir un modèle de Plateform as a Service (PaaS) suffisamment versatile qui permet l’interopérabilité entre des fournisseurs de clouds différents et des solutions auto-hébergées (on premise).
Cependant cette interopérabilité n’est pas automatique (pour les cas complexes) car Kubernetes permet beaucoup de variations. Concrêtement il existe des variations entre les installations possibles de Kubernetes
Distributions de Kubernetes Kubernetes est avant tout un ensemble de standards qui peuvent avoir des implémentations concurrentes. Il existe beaucoup de variétés (flavours) de Kubernetes, implémentant concrètement les solutions techniques derrière tout ce que Kubernetes ne fait que définir : solutions réseau, stockage (distribué ou non), loadbalancing, service de reverse proxy (Ingress), autoscaling de cluster (ajout de nouvelles VM au cluster automatiquement), monitoring…
Il est très possible de monter un cluster Kubernetes en dehors de ces fournisseurs, mais cela demande de faire des choix (ou bien une solution opinionated ouverte comme Rancher) et une relative maîtrise d’un nombre varié de sujets (bases de données, solutions de loadbalancing, redondance du stockage…).
C’est là le compromis de kubernetes : tout est ouvert et standardisé, mais devant la complexité et connaissance nécessaire pour mettre en place sa propre solution (stockage distribué par exemple) il est souvent préférable de louer un cluster chez un fournisseur quitte à retomber dans un certain vendor lock-in (enfermement propriétaire).
Quelques variantes connues de Kubernetes:
Google Kubernetes Engine (GKE) (Google Cloud Plateform): L’écosystème Kubernetes développé par Google. Très populaire car très flexible tout en étant l’implémentation de référence de Kubernetes. Azure Kubernetes Services (AKS) (Microsoft Azure): Un écosystème Kubernetes axé sur l’intégration avec les services du cloud Azure (stockage, registry, réseau, monitoring, services de calcul, loadbalancing, bases de données…). Elastic Kubernetes Services (EKS) (Amazon Web Services): Un écosystème Kubernetes assez standard à la sauce Amazon axé sur l’intégration avec le cloud Amazon (la gestion de l’accès, des loadbalancers ou du scaling notamment, le stockage avec Amazon EBS, etc.). Rancher: Un écosystème Kubernetes très complet, assez opinionated et entièrement open-source, non lié à un fournisseur de cloud. Inclut l’installation de stack de monitoring (Prometheus), de logging, de réseau mesh (Istio) via une interface web agréable. Rancher maintient aussi de nombreuses solutions open source, comme par exemple Longhorn pour le stockage distribué. K3S: Un écosystème Kubernetes fait par l’entreprise Rancher et axé sur la légèreté. Il remplace etcd par une base de données Postgres, utilise Traefik pour l’ingress et Klipper pour le loadbalancing. Openshift : Une version de Kubernetes configurée et optimisée par Red Hat pour être utilisée dans son écosystème. Tout est intégré donc plus guidé, avec l’inconvénient d’être un peu captif·ve de l’écosystème et des services vendus par Red Hat. `,description:"",tags:null,title:"Presentation de Kubernetes",uri:"/cours1_presentation_kubernetes/"},{content:`Architecture de Kubernetes Kubernetes master Le Master est responsable du maintien de l’état souhaité pour votre cluster.
Lorsque vous interagissez avec Kubernetes, par exemple en utilisant l’interface en ligne de commande kubectl, vous communiquez avec le master Kubernetes de votre cluster.
Le “master” fait référence à un ensemble de processus gérant l’état du cluster. Le master peut également être répliqué pour la disponibilité et la redondance.
Noeuds Kubernetes Les nœuds d’un cluster sont les machines (serveurs physiques, machines virtuelles, etc.) qui exécutent vos applications et vos workflows.
Le master node Kubernetes contrôle chaque noeuds; vous interagirez rarement directement avec les nœuds.
Pour utiliser Kubernetes, vous utilisez les objets de l’API Kubernetes pour décrire l’état souhaité de votre cluster: quelles applications ou autres processus que vous souhaitez exécuter, quelles images de conteneur elles utilisent, le nombre de réplicas, les ressources réseau et disque que vous mettez à disposition, et plus encore.
Vous définissez l’état souhaité en créant des objets à l’aide de l’API Kubernetes, généralement via l’interface en ligne de commande, kubectl. Vous pouvez également utiliser l’API Kubernetes directement pour interagir avec le cluster et définir ou modifier l’état souhaité.
Une fois que vous avez défini l’état souhaité, le plan de contrôle Kubernetes (control plane) permet de faire en sorte que l’état actuel du cluster corresponde à l’état souhaité. Pour ce faire, Kubernetes effectue automatiquement diverses tâches, telles que le démarrage ou le redémarrage de conteneurs, la mise à jour du nombre de replicas d’une application donnée, etc.
Le Control Plane Le control plane Kubernetes comprend un ensemble de processus en cours d’exécution sur votre cluster:
Le master Kubernetes est un ensemble de trois processus qui s’exécutent sur un seul nœud de votre cluster, désigné comme nœud maître (master node en anglais). Ces processus sont:
kube-apiserver: expose l’API pour parler au cluster kube-controller-manager: basé sur une boucle qui controlle en permanence l’état des resources et essaie de le corriger s’il n’est plus conforme. kube-scheduler: monitore les resources des différents workers, décide et cartographie ou doivent être créé les conteneur(Pods) Chaque nœud non maître de votre cluster exécute deux processus : kubelet, qui communique avec le Kubernetes master et controle la création et l’état des pods sur son noeud. kube-proxy, un proxy réseau reflétant les services réseau Kubernetes sur chaque nœud.
Les différentes parties du control plane Kubernetes, telles que les processus kube-controller-manager et kubelet, déterminent la manière dont Kubernetes communique avec votre cluster.
Le control plane conserve un enregistrement de tous les objets Kubernetes du système et exécute des boucles de contrôle continues pour gérer l’état de ces objets. À tout moment, les boucles de contrôle du control plane répondent aux modifications du cluster et permettent de faire en sorte que l’état réel de tous les objets du système corresponde à l’état souhaité que vous avez fourni.
Par exemple, lorsque vous utilisez l’API Kubernetes pour créer un objet Deployment, vous fournissez un nouvel état souhaité pour le système. Le control plane Kubernetes enregistre la création de cet objet et exécute vos instructions en lançant les applications requises et en les planifiant vers des nœuds de cluster, afin que l’état actuel du cluster corresponde à l’état souhaité.
Le client kubectl …Permet depuis sa machine de travail de contrôler le cluster avec une ligne de commande qui ressemble un peu à celle de Docker:
Lister les ressources Créer et supprimer les ressources Gérer les droits d’accès etc. Cet utilitaire s’installe avec un gestionnaire de paquet classique mais est souvent fourni directement par une distribution de développement de kubernetes.
Pour se connecter, kubectl a besoin de l’adresse de l’API Kubernetes, d’un nom d’utilisateur et d’un certificat.
Ces informations sont fournies sous forme d’un fichier YAML appelé kubeconfig Comme nous le verrons en TP ces informations sont généralement fournies directement par le fournisseur d’un cluster k8s (provider ou k8s de dev) Le fichier kubeconfig par défaut se trouve sur Linux à l’emplacement ~/.kube/config.
On peut aussi préciser la configuration au runtime comme ceci: kubectl --kubeconfig=fichier_kubeconfig.yaml <commandes_k8s>
Le même fichier kubeconfig peut stocker plusieurs configurations dans un fichier YAML :
Exemple :
apiVersion: v1 clusters: - cluster: certificate-authority: /home/jacky/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURKekNDQWcrZ0F3SUJBZ0lDQm5Vd0RRWUpLb1pJaHZjTkFRRUxCUUF3TXpFVk1CTUdBMVVFQ2hNTVJHbG4KYVhSaGJFOWpaV0Z1TVJvd0dBWURWUVFERXhGck9<clipped>3SCsxYmtGOHcxdWI5eHYyemdXU1F3NTdtdz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K server: https://5ba26bee-00f1-4088-ae11-22b6dd058c6e.k8s.ondigitalocean.com name: do-lon1-k8s-tp-cluster contexts: - context: cluster: minikube user: minikube name: minikube - context: cluster: do-lon1-k8s-tp-cluster user: do-lon1-k8s-tp-cluster-admin name: do-lon1-k8s-tp-cluster current-context: do-lon1-k8s-tp-cluster kind: Config preferences: {} users: - name: do-lon1-k8s-tp-cluster-admin user: token: 8b2d33e45b980c8642105ec827f41ad343e8185f6b4526a481e312822d634aa4 - name: minikube user: client-certificate: /home/jacky/.minikube/profiles/minikube/client.crt client-key: /home/jacky/.minikube/profiles/minikube/client.key Ce fichier déclare 2 clusters (un local, un distant), 2 contextes et 2 users.
Installation de développement Pour installer un cluster de développement :
Minikube, tourne dans Docker par défaut (ou dans des VMs) solution très pratique et vanilla: kind avec Docker Desktop depuis peu (dans une VM aussi) un cluster léger avec k3s, de Rancher (simple et utilisable en production/edge) Cluster dans le Cloud (managed cluster) Tous les principaux provider de cloud fournissent depuis plus ou moins longtemps des solutions de cluster gérées par eux :
Google Cloud Plateform avec Google Kubernetes Engine (GKE) : très populaire car très flexible et l’implémentation de référence de Kubernetes. AWS avec EKS : Kubernetes assez standard mais à la sauce Amazon pour la gestion de l’accès, des loadbalancers ou du scaling. Azure avec AKS : Kubernetes assez standard mais à la sauce Amazon pour la gestion de l’accès, des loadbalancers ou du scaling. DigitalOcean ou Scaleway : un peu moins de fonctions mais plus simple à appréhender Installer un cluster de production L’installation est décrite dans la documentation officielle
Opérer et maintenir un cluster de production Kubernetes “à la main” est très complexe et une tâche à ne pas prendre à la légère. De nombreux éléments doivent être installés et géré par les opérateurs.
Mise à jour et passage de version de kubernetes qui doit être fait très régulièrement car une version n’est supportée que 2 ans. Choix d’une configuration réseau et de sécurité adaptée. Installation probable de système de stockage distribué comme Ceph à maintenir également dans le temps Installer le dæmon Kubelet sur tous les noeuds Générer les bons certificats Installer un réseau CNI k8s comme flannel (d’autres sont possible et le choix vous revient) Déployer la base de données etcd Connecter les nœuds worker au master. … Installer un cluster complètement à la main pour s’exercer On peut également installer Kubernetes de façon encore plus manuelle pour mieux comprendre ses rouages et composants. Ce type d’installation est décrite par exemple ici : Kubernetes the hard way.
`,description:"",tags:null,title:"Installer Kubernetes",uri:"/cours2_installation_kubernetes/"},{content:`Découverte de Kubernetes Installer le client CLI kubectl kubectl est le point d’entré universel pour contrôler tous les type de cluster kubernetes. C’est un client en ligne de commande qui communique en REST avec l’API d’un cluster.
Nous allons explorer kubectl au fur et à mesure des TPs. Cependant à noter que :
kubectl peut gérer plusieurs clusters/configurations et switcher entre ces configurations kubectl n’est pas le seul outils pour intéragir avec les cluster, mais il est l’outil officiel. Il est recommender d’utiliser une version qui correspond à la version de votre cluster Kubernetes. La méthode d’installation importe peu. Vous pouvez suivre la methode officiel sur le site de Kubernetes
Faites kubectl version pour afficher la version du client kubectl. Bash completion Pour permettre à kubectl de compléter le nom des commandes et ressources avec <Tab> il est utile d’installer l’autocomplétion pour Bash :
sudo apt install bash-completion source <(kubectl completion bash) echo "source <(kubectl completion bash)" >> \${HOME}/.bashrc Vous pouvez désormais appuyer sur <Tab> pour compléter vos commandes kubectl, c’est très utile !
Installer Kubernetes Nous allons créer un cluster kubernetes en utilisant l’installeur RKE (Rancher Kubernetes Engine).
RKE est un installeur qui utilise docker pour contenir les différents composant de Kubernetes.
Il est donc nécessaire de disposer de machines où docker est installé et fonctionnel.
Preparation Nous allons utiliser cet article comme support.
Verifier les machines Leur donner un nom identifiable.
Verifier l’installation de Docker. Avoir un utilisateur en commun, cet utilisateur doit faire partie du groupe Docker.
Definir les accès SSH Créez une paire de clefs ssh sans passphrase sur le master (commande ssh-keygen) et placez les dans .ssh/id_rsa et .ssh/id_rsa.pub. Ne mettez pas de mot de passe.
Ajoutez la clef au fichier des clefs autorisées (.ssh/authorized_keys).
Attention de conserver les clefs déjà présentes (celles qui vous permettent de vous connecter à ces serveurs).
Vérifier le fonctionnement en testant la connexion depuis l’une des machines du cluster sur les autres.
Installation avec RKE RKE va nous simplifier la tache et va installer Kubernetes en fonction de nos paramètres spécifiés dans le fichier cluster.yaml
Récupérer la derniere version stable de RKE depuis le depot GitHub Le fichier qui est fourni est un simple fichier exécutable, vous pouvez changer ses droits et l’utilisez le avec la command:
./rke config Cette commande est un script interactif qui pose des questions pour créer le fichier de configuration du cluster.
Commencez par créer un cluster sur vos 3 machines, la première devant assurer le rôle de controle-plane et celui de etcd, les deux autres seront worker.
Il est possible de modifier le fichier après sa création. Vous trouverez dans la documentation officielle, un exemple court et un exemple avec toutes les options !
Attention, dans la plupart des cas, il faut utiliser les réponses par défaut. Cette commande va créer le fichier de configuration du cluster cluster.yml qu’il est possible d’éditer à la main si vous avez fait une petite erreur lors du script.
L’installation se lance simplement avec la commande:
./rke up --config cluster.yaml Si tout se passe bien, la commande se termine au bout de quelques minutes par
[...] INFO[0327] Finished building Kubernetes cluster successfully Explorons notre cluster k8s Notre cluster k8s est plein d’objets divers, organisés entre eux de façon dynamique pour décrire des applications, tâches de calcul, services et droits d’accès. La première étape consiste à explorer un peu le cluster :
Listez les nodes pour récupérer le nom de vos noeuds (kubectl get nodes) puis affichez ses caractéristiques avec kubectl describe node/<nom-du-noeud>. La commande get est générique et peut être utilisée pour récupérer la liste de tous les types de ressources.
De même, la commande describe peut s’appliquer à tout objet k8s. On doit cependant préfixer le nom de l’objet par son type (ex : node/<nom-du-noeud> ou nodes <nom-du-noeud>) car k8s ne peut pas deviner ce que l’on cherche quand plusieurs ressources ont le même nom.
Pour afficher tous les types de ressources à la fois que l’on utilise : kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 2m34s Il semble qu’il n’y a qu’une ressource dans notre cluster. Il s’agit du service d’API Kubernetes, pour qu’on puisse communiquer avec le cluster.
En réalité il y en a généralement d’autres cachés dans les autres namespaces.
En effet les éléments internes de Kubernetes tournent eux-mêmes sous forme de services et de daemons Kubernetes.
Les namespaces sont des groupes qui servent à isoler les ressources de façon logique et en termes de droits (avec le Role-Based Access Control (RBAC) de Kubernetes).
Pour vérifier cela on peut :
Afficher les namespaces : kubectl get namespaces Un cluster Kubernetes a généralement un namespace appelé default dans lequel les commandes sont lancées et les ressources créées si on ne précise rien.
Il a également aussi un namespace kube-system dans lequel résident les processus et ressources système de k8s.
Pour préciser le namespace on peut rajouter l’argument -n à la plupart des commandes k8s.
Pour lister les ressources liées au kubectl get all -n kube-system.
Ou encore : kubectl get all --all-namespaces (peut être abrégé en kubectl get all -A) qui permet d’afficher le contenu de tous les namespaces en même temps.
Pour avoir des informations sur un namespace : kubectl describe namespace/kube-system
Déployer une application en CLI Nous allons maintenant déployer une première application conteneurisée.
Le déploiement est un peu plus complexe qu’avec Docker, en particulier car il est séparé en plusieurs objets et plus configurable.
Pour créer un déploiement en ligne de commande (par opposition au mode déclaratif que nous verrons plus loin). On peut lancer par exemple: kubectl create deployment rancher-demo --image=monachus/rancher-demo Cette commande crée un objet de type deployment. Nous pourvons étudier ce deployment avec la commande kubectl describe deployment/rancher-demo.
Notez la liste des événements sur ce déploiement en bas de la description.
De la même façon que dans la partie précédente, listez les pods avec kubectl. Combien y en a-t-il ?
Agrandissons ce déploiement avec kubectl scale deployment rancher-demo --replicas=5
kubectl describe deployment/rancher-demo permet de constater que le service est bien passé à 5 replicas.
Observez à nouveau la liste des évènements, le scaling y est enregistré… Listez les pods pour constater A ce stade impossible d’afficher l’application : le déploiement n’est pas encore accessible de l’extérieur du cluster. Pour régler cela nous devons l’exposer grace à un service :
kubectl expose deployment rancher-demo --type=NodePort --port=8080 --name=rancher-demo-service Affichons la liste des services pour voir le résultat: kubectl get services Un service permet de créer un point d’accès unique exposant notre déploiement. Ici nous utilisons le type Nodeport car nous voulons que le service soit accessible de l’extérieur par l’intermédiaire d’un forwarding de port.
Sauriez-vous expliquer ce que l’app fait ? Pour le comprendre ou le confirmer, diminuez le nombre de réplicats à l’aide de la commande utilisée précédement pour passer à 10 réplicas. Qu se passe-t-il ? Une autre méthode pour accéder à un service (quel que soit sont type) en mode développement est de forwarder le traffic par l’intermédiaire de kubectl (et des composants kube-proxy installés sur chaque noeuds du cluster).
Pour cela on peut par exemple lancer: kubectl port-forward svc/rancher-demo-service 8080:8080 --address 127.0.0.1 Vous pouvez désormais accéder à votre app via via kubectl sur: http://localhost:8080. Quelle différence avec l’exposition précédente ? => Un seul conteneur s’affiche. En effet kubectl port-forward sert à créer une connexion de developpement/debug qui pointe toujours vers le même pod en arrière plan.
Pour exposer cette application en production sur un véritable cluster, nous devrions plutôt avoir recours à service de type un LoadBalancer. Mais RKE ne propose pas par défaut de loadbalancer. Nous y reviendrons dans le cours sur les objets kubernetes.
Simplifier les lignes de commande k8s Pour gagner du temps on dans les commandes Kubernetes on peut définir un alias: alias kc='kubectl' (à mettre dans votre .bash_profile en faisant echo "alias kc='kubectl'" >> ~/.bash_profile, puis en faisant source ~/.bash_profile).
Vous pouvez ensuite remplacer kubectl par kc dans les commandes.
Également pour gagner du temps en ligne de commande, la plupart des mots-clés de type Kubernetes peuvent être abrégés :
services devient svc deployments devient deploy etc. La liste complète : https://blog.heptio.com/kubectl-resource-short-names-heptioprotip-c8eff9fb7202
Essayez d’afficher les serviceaccounts (users) et les namespaces avec une commande courte. `,description:"",tags:null,title:"TD - Installation et Configuration",uri:"/td1-installation/"},{content:`L’API et les Objets Kubernetes Utiliser Kubernetes consiste à déclarer des objets grâce à l’API Kubernetes pour décrire l’état souhaité d’un cluster : quelles applications ou autres processus exécuter, quelles images elles utilisent, le nombre de replicas, les ressources réseau et disque que vous mettez à disposition, etc.
On définit des objets généralement via l’interface en ligne de commande et kubectl de deux façons :
en lançant une commande kubectl run <conteneur> ..., kubectl expose ... en décrivant un objet dans un fichier YAML ou JSON et en le passant au client kubectl apply -f monpod.yml Vous pouvez également écrire des programmes qui utilisent directement l’API Kubernetes pour interagir avec le cluster et définir ou modifier l’état souhaité. Kubernetes est complètement automatisable !
La commande apply Kubernetes encourage le principe de l’infrastructure-as-code : il est recommandé d’utiliser une description YAML et versionnée des objets et configurations Kubernetes plutôt que la CLI.
Pour cela la commande de base est kubectl apply -f object.yaml.
La commande inverse kubectl delete -f object.yaml permet de détruire un objet précédement appliqué dans le cluster à partir de sa description.
Lorsqu’on vient d’appliquer une description on peut l’afficher dans le terminal avec kubectl apply -f myobj.yaml view-last-applied
Globalement Kubernetes garde un historique de toutes les transformations des objets : on peut explorer, par exemple avec la commande kubectl rollout history deployment.
Parenthèse : Le YAML Kubernetes décrit ses ressources en YAML. A quoi ça ressemble, YAML ?
- marché: lieu: Marché de la Place jour: jeudi horaire: unité: "heure" min: 12 max: 20 fruits: - nom: pomme couleur: "verte" pesticide: avec - nom: poires couleur: jaune pesticide: sans légumes: - courgettes - salade - potiron Syntaxe Alignement ! (2 espaces !!)
ALIGNEMENT !! (comme en python)
ALIGNEMENT !!! (le défaut du YAML, pas de correcteur syntaxique automatique, c’est bête mais vous y perdrez forcément du temps !)
des listes (tirets)
des paires clé: valeur
Un peu comme du JSON, avec cette grosse différence que le JSON se fiche de l’alignement et met des accolades et des points-virgules
les extensions Kubernetes et YAML dans VSCode vous aident à repérer des erreurs
Syntaxe de base d’une description YAML Kubernetes Les descriptions YAML permettent de décrire de façon lisible et manipulable de nombreuses caractéristiques des ressources Kubernetes (un peu comme un Compose file par rapport à la CLI Docker).
Exemple Création d’un service simple :
kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard type: NodePort Remarques de syntaxe :
Toutes les descriptions doivent commencer par spécifier la version d’API (minimale) selon laquelle les objets sont censés être créés Il faut également préciser le type d’objet avec kind Le nom dans metadata:\\n name: value est également obligatoire. On rajoute généralement une description longue démarrant par spec: Description de plusieurs ressources On peut mettre plusieurs ressources à la suite dans un fichier k8s : cela permet de décrire une installation complexe en un seul fichier
par exemple le dashboard Kubernetes https://github.com/kubernetes/dashboard/ L’ordre n’importe pas car les ressources sont décrites déclarativement c’est-à-dire que:
Les dépendances entre les ressources sont déclarées Le control plane de Kubernetes se charge de planifier l’ordre correct de création en fonction des dépendances (pods avant le déploiement, rôle avec l’utilisateur lié au rôle) On préfère cependant les mettre dans un ordre logique pour que les humains puissent les lire. On peut sauter des lignes dans le YAML et rendre plus lisible les descriptions
On sépare les différents objets par ---
Objets de base Les namespaces Tous les objets Kubernetes sont rangés dans différents espaces de travail isolés appelés namespaces.
Cette isolation permet 3 choses :
ne voir que ce qui concerne une tâche particulière (ne réfléchir que sur une seule chose lorsqu’on opère sur un cluster) créer des limites de ressources (CPU, RAM, etc.) pour le namespace définir des rôles et permissions sur le namespace qui s’appliquent à toutes les ressources à l’intérieur. Lorsqu’on lit ou créé des objets sans préciser le namespace, ces objets sont liés au namespace default.
Pour utiliser un namespace autre que default avec kubectl il faut :
le préciser avec l’option -n : kubectl get pods -n kube-system créer une nouvelle configuration dans la kubeconfig pour changer le namespace par defaut. Kubernetes gère lui-même ses composants internes sous forme de pods et services.
Si vous ne trouvez pas un objet, essayez de lancer la commande kubectl avec l’option -A ou --all-namespaces Les Pods Un Pod est l’unité d’exécution de base d’une application Kubernetes que vous créez ou déployez. Un Pod représente des process en cours d’exécution dans votre Cluster.
Un Pod encapsule un conteneur (ou souvent plusieurs conteneurs), des ressources de stockage, une IP réseau unique, et des options qui contrôlent comment le ou les conteneurs doivent s’exécuter (ex: restart policy). Cette collection de conteneurs et volumes tournent dans le même environnement d’exécution mais les processus sont isolés.
Un Pod représente une unité de déploiement : un petit nombre de conteneurs qui sont étroitement liés et qui partagent :
les mêmes ressources de calcul des volumes communs la même IP donc le même nom de domaine peuvent se parler sur localhost peuvent se parler en IPC ont un nom différent et des logs différents Chaque Pod est destiné à exécuter une instance unique d’un workload donné. Si vous désirez mettre à l’échelle votre workload, vous devez multiplier le nombre de Pods avec un déploiement.
Pour plus de détail sur la philosophie des pods, vous pouvez consulter ce bon article.
Kubernetes fournit un ensemble de commande pour débugger des conteneurs :
kubectl logs <pod-name> -c <conteneur_name> (le nom du conteneur est inutile si un seul) kubectl exec -it <pod-name> -c <conteneur_name> -- bash kubectl attach -it <pod-name> Enfin, pour debugger la sortie réseau d’un programme on peut rapidement forwarder un port depuis un pods vers l’extérieur du cluster :
kubectl port-forward <pod-name> <port_interne>:<port_externe> C’est une commande de debug seulement : pour exposer correctement des processus k8s, il faut créer un service, par exemple avec NodePort. Pour copier un fichier dans un pod on peut utiliser: kubectl cp <pod-name>:</path/to/remote/file> </path/to/local/file>
Pour monitorer rapidement les ressources consommées par un ensemble de processus il existe les commande kubectl top nodes et kubectl top pods
Un manifest de Pod rancher-demo-pod.yaml
apiVersion: v1 kind: Pod metadata: name: rancher-demo-pod spec: containers: - image: monachus/rancher-demo:latest name: rancher-demo-container ports: - containerPort: 8080 name: http protocol: TCP - image: redis name: redis-container ports: - containerPort: 6379 name: http protocol: TCP Rappel sur quelques concepts Haute disponibilité Faire en sorte qu’un service ait un “uptime” élevé. On veut que le service soit tout le temps accessible même lorsque certaines ressources manquent :
elles tombent en panne elles sont sorties du service pour mise à jour, maintenance ou modification Pour cela on doit avoir des ressources multiples…
Plusieurs serveurs Plusieurs versions des données Plusieurs accès réseau Il faut que les ressources disponibles prennent automatiquement le relais des ressources indisponibles. Pour cela on utilise en particulier:
des “load balancers” : aiguillages réseau intelligents des “healthchecks” : une vérification de la santé des applications Nous allons voir que Kubernetes intègre automatiquement les principes de load balancing et de healthcheck dans l’orchestration de conteneurs
Répartition de charge (load balancing) Un load balancer : une sorte d’“aiguillage” de trafic réseau, typiquement HTTP(S) ou TCP. Un aiguillage intelligent qui se renseigne sur plusieurs critères avant de choisir la direction. Cas d’usage :
Éviter la surcharge : les requêtes sont réparties sur différents backends pour éviter de les saturer. L’objectif est de permettre la haute disponibilité : on veut que notre service soit toujours disponible, même en période de panne/maintenance.
Donc on va dupliquer chaque partie de notre service et mettre les différentes instances derrière un load balancer.
Le load balancer va vérifier pour chaque backend s’il est disponible (healthcheck) avant de rediriger le trafic.
Répartition géographique : en fonction de la provenance des requêtes on va rediriger vers un datacenter adapté (+ proche).
Healthchecks Fournir à l’application une façon d’indiquer qu’elle est disponible, c’est-à-dire :
qu’elle est démarrée (liveness) qu’elle peut répondre aux requêtes (readiness). Application microservices Une application composée de nombreux petits services communiquant via le réseau. Le calcul pour répondre à une requête est décomposé en différente parties distribuées entre les services. Par exemple:
un service est responsable de la gestion des clients et un autre de la gestion des commandes.
Ce mode de développement implique souvent des architectures complexes pour être mis en oeuvre et kubernetes est pensé pour faciliter leur gestion à grande échelle.
Imaginez devoir relancer manuellement des services vitaux pour une application en hébergeant des centaines d’instances : c’est en particulier à ce moment que kubernetes devient indispensable.
2 exemples d’application microservices: https://github.com/microservices-patterns/ftgo-application -> fonctionne avec le très bon livre Microservices pattern visible sur le readme. https://github.com/GoogleCloudPlatform/microservices-demo -> Exemple d’application microservice de référence de Google pour Kubernetes. L’architecture découplée des services Kubernetes Comme nous l’avons vu dans le TD precedent, déployer une application dans kubernetes demande plusieurs étapes. En réalité en plus des pods l’ensemble de la gestion d’un service applicatif se décompose dans Kubernetes en 3 à 4 objets articulés entre eux:
replicatset deployment service (ingress) Les Deployments (deploy) Les déploiements sont les objets effectivement créés manuellement lorsqu’on déploie une application. Ce sont des objets de plus haut niveau que les pods et replicaset et les pilote pour gérer un déploiement applicatif.
Les poupées russes Kubernetes : un Deployment contient un ReplicaSet, qui contient des Pods, qui contiennent des conteneurs
S’il c’est nécessaire d’avoir ces trois types de ressources c’est parce que Kubernetes respecte un principe de découplage des responsabilités.
La responsabilité d’un déploiement est de gérer la coexistence et le tracking de versions multiples d’une application et d’effectuer des montées de version automatiques en haute disponibilité en suivant une RolloutStrategy.
Ainsi lors des changements de version, un seul deployment gère automatiquement de multiples replicasets contenant chacun une version de l’application => Le découplage est nécessaire.
Un deployment implique la création d’un ensemble de Pods désignés par une étiquette label et regroupé dans un Replicaset.
Exemple :
apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 strategy: type: Recreate selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 Pour les afficher : kubectl get deployments
La commande kubectl run sert à créer un deployment à partir d’un modèle. Il vaut mieux utilisez apply -f.
Les ReplicaSets (rs) Dans notre modèle, les ReplicaSet servent à gérer et sont responsables pour:
la réplication (avoir le bon nombre d’instances et le scaling)
la santé et le redémarrage automatique des pods de l’application (Self-Healing)
kubectl get rs pour afficher la liste des replicas.
En général on ne les manipule pas directement (c’est déconseillé) même s’il est possible de les modifier et de les créer avec un fichier de ressource. Pour créer des groupes de conteneurs on utilise soit un Deployment soit d’autres formes de workloads (DaemonSet, StatefulSet, Job) adaptés à d’autres cas.
Les Services Dans Kubernetes, un service est un objet qui :
Désigne un ensemble de pods (grâce à des tags) généralement géré par un déploiement. Fournit un endpoint réseau pour les requêtes à destination de ces pods. Configure une politique permettant d’y accéder depuis l’intérieur ou l’extérieur du cluster. L’ensemble des pods ciblés par un service est déterminé par un selector.
Par exemple, considérons un backend de traitement d’image (stateless, c’est-à-dire ici sans base de données) qui s’exécute avec 3 replicas. Ces replicas sont interchangeables et les frontends ne se soucient pas du backend qu’ils utilisent. Bien que les pods réels qui composent l’ensemble backend puissent changer, les clients frontends ne devraient pas avoir besoin de le savoir, pas plus qu’ils ne doivent suivre eux-mêmes l’état de l’ensemble des backends.
L’abstraction du service permet ce découplage : les clients frontend s’addressent à une seule IP avec un seul port dès qu’ils ont besoin d’avoir recours à un backend. Les backends vont recevoir la requête du frontend aléatoirement.
Les Services sont de trois types principaux :
ClusterIP: expose le service sur une IP interne au cluster. Les autres pods peuvent alors accéder au service de l’intérieur du cluster, mais il n’est pas l’extérieur.
NodePort: expose le service depuis l’IP de chacun des noeuds du cluster en ouvrant un port directement sur le nœud, entre 30000 et 32767. Cela permet d’accéder aux pods internes répliqués. Comme l’IP est stable on peut faire pointer un DNS ou Loadbalancer classique dessus.
LoadBalancer: expose le service en externe à l’aide d’un Loadbalancer de fournisseur de cloud. Les services NodePort et ClusterIP, vers lesquels le Loadbalancer est dirigé sont automatiquement créés. Les autres types de Workloads Kubernetes En plus du déploiement d’un application, Il existe pleins d’autre raisons de créer un ensemble de Pods:
Le DaemonSet: Faire tourner un agent ou démon sur chaque nœud, par exemple pour des besoins de monitoring, ou pour configurer le réseau sur chacun des nœuds. Le Job : Effectuer une tache unique de durée limitée et ponctuelle, par exemple de nettoyage d’un volume ou la préparation initiale d’une application, etc. Le CronJob : Effectuer une tache unique de durée limitée et récurrente, par exemple de backup ou de régénération de certificat, etc. De plus même pour faire tourner une application, les déploiements ne sont pas toujours suffisants. En effet ils sont peu adaptés à des applications statefull comme les bases de données de toutes sortes qui ont besoin de persister des données critiques. Pour celà on utilise un StatefulSet que nous verrons par la suite.
Étant donné les similitudes entre les DaemonSets, les StatefulSets et les Deployments, il est important de comprendre un peu précisément quand les utiliser.
Les Deployments (liés à des ReplicaSets) doivent être utilisés :
lorsque votre application est complètement découplée du nœud que vous pouvez en exécuter plusieurs copies sur un nœud donné sans considération particulière que l’ordre de création des replicas et le nom des pods n’est pas important lorsqu’on fait des opérations stateless Les DaemonSets doivent être utilisés :
lorsqu’au moins une copie de votre application doit être exécutée sur tous les nœuds du cluster (ou sur un sous-ensemble de ces nœuds). Les StatefulSets doivent être utilisés :
lorsque l’ordre de création des replicas et le nom des pods est important lorsqu’on fait des opérations stateful (écrire dans une base de données) Jobs Les jobs sont utiles pour les choses que vous ne voulez faire qu’une seule fois, comme les migrations de bases de données ou les travaux par lots. Si vous exécutez une migration en tant que Pod dans un deployment:
Dès que la migration se finit le processus du pod s’arrête. Le replicaset qui détecte que l’“application” s’est arrêter va tenter de la redémarrer en recréant le pod. Votre tâche de migration de base de données se déroulera donc en boucle, en repeuplant continuellement la base de données. CronJobs Comme des jobs, mais se lancent à un intervalle régulier, comme les cron sur les systèmes unix.
`,description:"",tags:null,title:"Les objets Kubernetes - Partie 1",uri:"/cours3_objets_kubernetes_partie1/"},{content:`Déployer Wordpress et MySQL avec du stockage et des Secrets Nous allons suivre ce tutoriel pas à pas : https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/
Il faut :
copier les 2 fichiers et les appliquer vérifier que le stockage a bien fonctionné découvrir ce qui manque pour que cela fonctionne le créer à la main ou suivre le reste du tutoriel qui passe par l’outil Kustomize (attention, Kustomize ajoute un suffixe aux ressources qu’il créé) On peut ensuite observer les différents objets créés, et optimiser le process avec un fichier kustomization.yaml plus complet.
Entrez dans un des pods, et de l’intérieur, lisez le secret qui lui a été rendu accessible. Facultatif : la stack Wordsmith Etudions et lançons ensemble cette application:
https://github.com/dockersamples/wordsmith
`,description:"",tags:null,title:"TD - Deployer Wordpress",uri:"/td2-deployer-wordpress/"},{content:"",description:"",tags:null,title:"Categories",uri:"/categories/"},{content:`Home Afin de mener à bien le TP, vous aurez besoin de 3 VMs.
L’objectif de ce TP est d’orchestrer des containers pour améliorer la disponibilité et la scalabilité des applications.
La haute disponibilité du “control plane” de Kubernetess ne sera pas traitée ici.
Compétences pré-requises:
Construire un container avec un Dockerfile Utiliser un registry Les commandes Docker L’objectif de ce TP est d’orchestrer des containers pour améliorer la disponibilité et la scalabilité des applications. La haute disponibilité du “control plane” de k8s ne sera pas traitée ici.
Les compétences visées:
travailler avec une configuration Kubernetes utiliser la commande “kubectl” transformer les commandes “imperative” en “declarative” utiliser Docker et Kubernetes créer et gérer un “pod” créer et gérer un “pod init” limiter la consommation de ressources d’un pod utiliser un label utiliser les concepts de “taint & toleration” mettre un noeud k8s en maintenance et le remettre en production générer une définition en “Yaml” d’un objet k8s à partir d’un “dry-run” “kubernétiser” une image Docker rendre disponible son application en dehors du Cluster (services dont “NodePort” et “LoadBalancer”, “ingress controller”, “Loadbalancer”,“Ingress Controller”) gérer le cycle de vie d’une application dans Kubernetes utiliser le manager d’applications “HELM” stocker des données de façon permanente dans un cluster Amusez-vous !
`,description:"",tags:null,title:"Home",uri:"/"},{content:"",description:"",tags:null,title:"Tags",uri:"/tags/"}]